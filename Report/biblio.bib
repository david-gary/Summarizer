@inproceedings{Transformers,
  author    = {Guan, Wang and Smetannikov, Ivan and Tianxing, Man},
  title     = {Survey on Automatic Text Summarization and Transformer Models Applicability},
  year      = {2020},
  isbn      = {9781450388054},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3437802.3437832},
  doi       = {10.1145/3437802.3437832},
  booktitle = {2020 International Conference on Control, Robotics and Intelligent System},
  pages     = {176–184},
  numpages  = {9},
  keywords  = {Automatic Text Summarization, Pre-trained language model, Natural Language Processing},
  location  = {Xiamen, China},
  series    = {CCRIS 2020}
}

@article{Pyramid,
  author     = {Nenkova, Ani and Passonneau, Rebecca and McKeown, Kathleen},
  title      = {The Pyramid Method: Incorporating Human Content Selection Variation in Summarization Evaluation},
  year       = {2007},
  issue_date = {May 2007},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {4},
  number     = {2},
  issn       = {1550-4875},
  url        = {https://doi.org/10.1145/1233912.1233913},
  doi        = {10.1145/1233912.1233913},
  abstract   = {Human variation in content selection in summarization has given rise to some fundamental research questions: How can one incorporate the observed variation in suitable evaluation measures? How can such measures reflect the fact that summaries conveying different content can be equally good and informative? In this article, we address these very questions by proposing a method for analysis of multiple human abstracts into semantic content units. Such analysis allows us not only to quantify human variation in content selection, but also to assign empirical importance weight to different content units. It serves as the basis for an evaluation method, the Pyramid Method, that incorporates the observed variation and is predictive of different equally informative summaries. We discuss the reliability of content unit annotation, the properties of Pyramid scores, and their correlation with other evaluation methods.},
  journal    = {ACM Trans. Speech Lang. Process.},
  month      = {may},
  pages      = {4–es},
  numpages   = {23},
  keywords   = {summarization, Evaluation, semantic analysis}
}

@inproceedings{Rouge,
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  author    = {Lin, Chin-Yew},
  booktitle = {Text Summarization Branches Out},
  month     = jul,
  year      = {2004},
  address   = {Barcelona, Spain},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W04-1013},
  pages     = {74--81}
}